{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a860ad6",
   "metadata": {},
   "source": [
    "########## THEORY QUESTIONS ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe26b3b",
   "metadata": {},
   "source": [
    "Question = 1 >>> What is Generative AI and what are its primary use cases across industries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff5e9",
   "metadata": {},
   "source": [
    "Ans = Generative Artificial Intelligence (Generative AI) is a type of AI that can create new and original content such as text, images, audio, video, code, and data by learning patterns from large amounts of existing data.\n",
    "\n",
    " Unlike traditional AI, which mainly analyzes or classifies data, Generative AI produces new content that is similar to human-created content.\n",
    "\n",
    "Simple Definition (Exam-friendly)\n",
    "\n",
    "Generative AI is an AI technology that generates new content by learning patterns from existing data.\n",
    "\n",
    "Primary Use Cases of Generative AI Across Industries\n",
    "1. Healthcare\n",
    "\n",
    "Medical report generation\n",
    "\n",
    "Clinical documentation\n",
    "\n",
    "Drug discovery and molecule design\n",
    "\n",
    "Medical image enhancement\n",
    "\n",
    " Example: AI generating patient lab reports or summarizing medical records.\n",
    "\n",
    "2. Education\n",
    "\n",
    "Automatic note generation\n",
    "\n",
    "Personalized learning content\n",
    "\n",
    "Question paper and quiz creation\n",
    "\n",
    "Virtual tutors\n",
    "\n",
    " Example: AI explaining complex topics in simple language.\n",
    "\n",
    "3. Marketing & Advertising\n",
    "\n",
    "Content writing (blogs, ads, slogans)\n",
    "\n",
    "Social media post generation\n",
    "\n",
    "Personalized campaigns\n",
    "\n",
    "Brand storytelling\n",
    "\n",
    " Example: AI creating promotional content for products.\n",
    "\n",
    "4. Software & IT Industry\n",
    "\n",
    "Code generation and debugging\n",
    "\n",
    "Software documentation\n",
    "\n",
    "Test case generation\n",
    "\n",
    "Chatbots and virtual assistants\n",
    "\n",
    " Example: GitHub Copilot suggesting code automatically.\n",
    "\n",
    "5. Media & Entertainment\n",
    "\n",
    "Music composition\n",
    "\n",
    "Video and animation generation\n",
    "\n",
    "Script writing\n",
    "\n",
    "Game content creation\n",
    "\n",
    " Example: AI-generated background music for videos.\n",
    "\n",
    "6. Finance\n",
    "\n",
    "Automated financial reports\n",
    "\n",
    "Fraud detection support\n",
    "\n",
    "Risk analysis summaries\n",
    "\n",
    "Customer service chatbots\n",
    "\n",
    " Example: AI generating monthly financial summaries.\n",
    "\n",
    "7. Manufacturing & Engineering\n",
    "\n",
    "Product design optimization\n",
    "\n",
    "Simulation data generation\n",
    "\n",
    "Predictive maintenance reports\n",
    "\n",
    " Example: AI designing optimized machine parts.\n",
    "\n",
    "8. Retail & E-commerce\n",
    "\n",
    "Product descriptions\n",
    "\n",
    "Personalized recommendations\n",
    "\n",
    "Customer support automation\n",
    "\n",
    " Example: AI generating customized shopping suggestions.\n",
    "\n",
    "Advantages of Generative AI\n",
    "\n",
    "Saves time and cost\n",
    "\n",
    "Improves productivity\n",
    "\n",
    "Enables creativity\n",
    "\n",
    "Works at large scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678d4bc",
   "metadata": {},
   "source": [
    "Question = 2 >>> Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb1a9c",
   "metadata": {},
   "source": [
    "Ans = Probabilistic modeling is the foundation of generative models. It allows the model to learn the probability distribution of data and generate new samples from it.\n",
    "\n",
    "üîπ What does probabilistic modeling mean?\n",
    "\n",
    "It means representing data using probability distributions instead of fixed rules.\n",
    "\n",
    "üîπ How probabilistic modeling works in generative models\n",
    "\n",
    "Learning data distribution\n",
    "Generative models learn the joint probability distribution\n",
    "\n",
    "ùëÉ\n",
    "(\n",
    "ùëã\n",
    ",\n",
    "ùëå\n",
    ")\n",
    "P(X,Y)\n",
    "\n",
    "where:\n",
    "\n",
    "ùëã\n",
    "X = input data (text, image, audio)\n",
    "\n",
    "ùëå\n",
    "Y = labels (optional)\n",
    "\n",
    "Capturing uncertainty\n",
    "Real-world data is uncertain and noisy. Probabilistic models handle:\n",
    "\n",
    "Missing data\n",
    "\n",
    "Variability\n",
    "\n",
    "Multiple possible outputs\n",
    "\n",
    "Sampling new data\n",
    "Once the probability distribution is learned, the model can sample from it to create new data points.\n",
    "\n",
    "üìå Example:\n",
    "A text-generation model predicts the probability of the next word given previous words and samples from that distribution.\n",
    "\n",
    "üîπ Why probabilistic modeling is important\n",
    "\n",
    "Enables data generation\n",
    "\n",
    "Handles uncertainty\n",
    "\n",
    "Allows multiple possible outputs\n",
    "\n",
    "Makes models flexible and realistic\n",
    "\n",
    "2. Difference Between Generative and Discriminative Models\n",
    "Feature\tGenerative Models\tDiscriminative Models\n",
    "Probability learned\tJoint probability \n",
    "ùëÉ\n",
    "(\n",
    "ùëã\n",
    ",\n",
    "ùëå\n",
    ")\n",
    "P(X,Y)\tConditional probability \n",
    "ùëÉ\n",
    "(\n",
    "ùëå\n",
    "‚à£\n",
    "ùëã\n",
    ")\n",
    "P(Y‚à£X)\n",
    "Main goal\tGenerate new data\tClassify or predict labels\n",
    "Can create data?\t‚úÖ Yes\t‚ùå No\n",
    "Handles missing data\tBetter\tLimited\n",
    "Output type\tSamples, images, text\tLabels or probabilities\n",
    "Example models\tGPT, GANs, VAEs, Naive Bayes\tLogistic Regression, SVM, CNN classifier\n",
    "\n",
    "üîπ Simple Example\n",
    "\n",
    "Generative model: Learns how handwritten digits look and can generate new digits.\n",
    "\n",
    "Discriminative model: Learns to identify which digit (0‚Äì9) an image represents.\n",
    "\n",
    "3. Key Concept Summary\n",
    "\n",
    "Generative models model how data is produced\n",
    "\n",
    "Discriminative models separate or classify data\n",
    "\n",
    "Probabilistic modeling enables sampling, uncertainty handling, and realism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2b9e8",
   "metadata": {},
   "source": [
    "Question = 3 >>> What is the difference between Autoencoders and Variational Autoencoders (VAEs) in the context of text generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b8281",
   "metadata": {},
   "source": [
    "Ans = 1. Autoencoders (AE)\n",
    "üîπ What is an Autoencoder?\n",
    "\n",
    "An Autoencoder is a neural network that compresses input text into a latent representation and then reconstructs the same text from it.\n",
    "\n",
    "üîπ Architecture\n",
    "\n",
    "Encoder: Converts text ‚Üí latent vector\n",
    "\n",
    "Decoder: Reconstructs text from latent vector\n",
    "\n",
    "üîπ In text generation\n",
    "\n",
    "Mainly used for text compression, denoising, and feature extraction\n",
    "\n",
    "Not ideal for generating new text, because:\n",
    "\n",
    "Latent space is not structured\n",
    "\n",
    "Cannot easily sample new meaningful sentences\n",
    "\n",
    "üîπ Limitations\n",
    "\n",
    "Deterministic (same input ‚Üí same output)\n",
    "\n",
    "No probability distribution in latent space\n",
    "\n",
    "Poor creativity in text generation\n",
    "\n",
    "2. Variational Autoencoders (VAE)\n",
    "üîπ What is a VAE?\n",
    "\n",
    "A Variational Autoencoder is a probabilistic generative model that learns a distribution over the latent space, not just fixed vectors.\n",
    "\n",
    "üîπ Architecture\n",
    "\n",
    "Encoder: Outputs mean (Œº) and variance (œÉ¬≤)\n",
    "\n",
    "Latent space: Sampled using probability distribution\n",
    "\n",
    "Decoder: Generates text from sampled latent vector\n",
    "\n",
    "üîπ In text generation\n",
    "\n",
    "Can generate new and diverse sentences\n",
    "\n",
    "Enables smooth interpolation between sentences\n",
    "\n",
    "Handles uncertainty and variability in language\n",
    "\n",
    "3. Key Differences (Table)\n",
    "\n",
    "Feature                 \tAutoencoder (AE)\t                  Variational Autoencoder (VAE)\n",
    "\n",
    "Latent space\t            Fixed, deterministic\t              Probabilistic, structured\n",
    "\n",
    "Probability modeling\t    ‚ùå No\t                              ‚úÖ Yes\n",
    "\n",
    "Sampling new text       \t‚ùå Difficult\t                          ‚úÖ Easy\n",
    "\n",
    "Diversity in output     \tLow\t                                    High\n",
    "\n",
    "Creativity\t                Limited\t                                Better\n",
    "\n",
    "Used for\t                Reconstruction\t                        Generation + reconstruction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71eb1c",
   "metadata": {},
   "source": [
    "Question = 4 >>> Describe the working of attention mechanisms in Neural Machine\n",
    "Translation (NMT). Why are they critical?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdfe75",
   "metadata": {},
   "source": [
    "Ans = Neural Machine Translation (NMT) uses neural networks to translate text from a source language to a target language.\n",
    "\n",
    "üìå Example:\n",
    "English ‚Üí Hindi\n",
    "\n",
    "‚ÄúI am learning AI‚Äù ‚Üí ‚Äú‡§Æ‡•à‡§Ç AI ‡§∏‡•Ä‡§ñ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‚Äù\n",
    "\n",
    "2. What is an Attention Mechanism?\n",
    "\n",
    "The attention mechanism allows the model to focus on relevant words in the source sentence while generating each word in the target sentence.\n",
    "\n",
    "üëâ Instead of relying on a single fixed representation, attention dynamically selects important parts of the input.\n",
    "\n",
    "3. Working of Attention Mechanism (Step-by-Step)\n",
    "üîπ Step 1: Encoding the Source Sentence\n",
    "\n",
    "The encoder (RNN/LSTM/GRU) converts each source word into a hidden state.\n",
    "\n",
    "This produces a sequence of hidden states:\n",
    "\n",
    "‚Ñé\n",
    "1\n",
    ",\n",
    "‚Ñé\n",
    "2\n",
    ",\n",
    "‚Ñé\n",
    "3\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "‚Ñé\n",
    "ùëõ\n",
    "h\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    ",h\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    ",h\n",
    "3\n",
    "\t‚Äã\n",
    "\n",
    ",...,h\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "üîπ Step 2: Decoding with Attention\n",
    "\n",
    "At each decoding step, the decoder:\n",
    "\n",
    "Generates one target word at a time\n",
    "\n",
    "Uses attention to decide which source words are most relevant\n",
    "\n",
    "üîπ Step 3: Attention Score Calculation\n",
    "\n",
    "The decoder computes alignment scores between:\n",
    "\n",
    "Current decoder state\n",
    "\n",
    "Each encoder hidden state\n",
    "\n",
    "These scores measure relevance.\n",
    "\n",
    "üîπ Step 4: Attention Weights (Softmax)\n",
    "\n",
    "The scores are normalized using softmax to produce attention weights:\n",
    "\n",
    "ùõº\n",
    "1\n",
    ",\n",
    "ùõº\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "ùõº\n",
    "ùëõ\n",
    "Œ±\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    ",Œ±\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    ",...,Œ±\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "Each weight shows how much importance a source word has.\n",
    "\n",
    "üîπ Step 5: Context Vector Creation\n",
    "\n",
    "A context vector is computed as a weighted sum of encoder states:\n",
    "\n",
    "ùëê\n",
    "=\n",
    "‚àë\n",
    "ùõº\n",
    "ùëñ\n",
    "‚Ñé\n",
    "ùëñ\n",
    "c=‚àëŒ±\n",
    "i\n",
    "\t‚Äã\n",
    "\n",
    "h\n",
    "i\n",
    "\t‚Äã\n",
    "\n",
    "üîπ Step 6: Target Word Generation\n",
    "\n",
    "The decoder combines:\n",
    "\n",
    "Context vector\n",
    "\n",
    "Decoder hidden state\n",
    "\n",
    "to predict the next target word.\n",
    "\n",
    "4. Why Attention is Critical in NMT\n",
    "‚úÖ Solves Long Sentence Problem\n",
    "\n",
    "Traditional encoder‚Äìdecoder models compress entire sentences into one vector.\n",
    "\n",
    "Attention avoids information loss.\n",
    "\n",
    "‚úÖ Improves Translation Quality\n",
    "\n",
    "Correct word alignment between languages.\n",
    "\n",
    "Better handling of word order differences.\n",
    "\n",
    "‚úÖ Enables Interpretability\n",
    "\n",
    "Attention weights show which source words influenced each translation step.\n",
    "\n",
    "‚úÖ Foundation for Transformers\n",
    "\n",
    "Self-attention completely replaces RNNs in modern models like Transformer, BERT, GPT.\n",
    "\n",
    "5. Simple Example\n",
    "\n",
    "English: ‚ÄúThe cat sat on the mat‚Äù\n",
    "Hindi: ‚Äú‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ö‡§ü‡§æ‡§à ‡§™‡§∞ ‡§¨‡•à‡§†‡•Ä‚Äù\n",
    "\n",
    "When generating ‚Äú‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä‚Äù, attention focuses on ‚Äúcat‚Äù\n",
    "When generating ‚Äú‡§ö‡§ü‡§æ‡§à‚Äù, attention focuses on ‚Äúmat‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0713a",
   "metadata": {},
   "source": [
    "Question = 5 >>>  What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159d5f4",
   "metadata": {},
   "source": [
    "Ans = Ethical Considerations in Using Generative AI for Creative Content\n",
    "\n",
    "(Poetry, Storytelling, Art, etc.)**\n",
    "\n",
    "1. Authorship and Ownership\n",
    "\n",
    "Who owns AI-generated poetry or stories?\n",
    "\n",
    "Is the creator the user, the AI developer, or the model itself?\n",
    "\n",
    "üìå Concern: Lack of clear copyright laws for AI-generated content.\n",
    "\n",
    "2. Originality and Plagiarism\n",
    "\n",
    "AI may generate content similar to existing works\n",
    "\n",
    "Risk of unintentional plagiarism\n",
    "\n",
    "üìå Concern: AI could mimic a known author‚Äôs style too closely.\n",
    "\n",
    "3. Bias and Representation\n",
    "\n",
    "AI learns from existing data, which may contain:\n",
    "\n",
    "Cultural bias\n",
    "\n",
    "Gender stereotypes\n",
    "\n",
    "Social prejudice\n",
    "\n",
    "üìå Concern: Stories may reinforce harmful stereotypes.\n",
    "\n",
    "4. Transparency and Disclosure\n",
    "\n",
    "Audiences should know whether content is AI-generated or human-written\n",
    "\n",
    "üìå Concern: Misleading readers about the source of creative work.\n",
    "\n",
    "5. Cultural Sensitivity\n",
    "\n",
    "AI may misuse or misrepresent:\n",
    "\n",
    "Cultural traditions\n",
    "\n",
    "Folklore\n",
    "\n",
    "Religious beliefs\n",
    "\n",
    "üìå Concern: Offending communities or spreading misinformation.\n",
    "\n",
    "6. Impact on Human Creativity\n",
    "\n",
    "Over-reliance on AI may:\n",
    "\n",
    "Reduce human creativity\n",
    "\n",
    "Devalue human artists and writers\n",
    "\n",
    "üìå Concern: AI replacing human creative jobs.\n",
    "\n",
    "7. Misuse and Harmful Content\n",
    "\n",
    "AI-generated stories could be used for:\n",
    "\n",
    "Propaganda\n",
    "\n",
    "Misinformation\n",
    "\n",
    "Hate speech\n",
    "\n",
    "üìå Concern: Ethical responsibility to prevent harmful outputs.\n",
    "\n",
    "8. Data Privacy\n",
    "\n",
    "Training data may include personal or sensitive information\n",
    "\n",
    "üìå Concern: Violation of privacy and data protection laws.\n",
    "\n",
    "9. Accountability\n",
    "\n",
    "If AI generates offensive or harmful content:\n",
    "\n",
    "Who is responsible?\n",
    "\n",
    "üìå Concern: Lack of clear accountability frameworks.\n",
    "\n",
    "10. Consent of Artists\n",
    "\n",
    "Artists and writers may not have consented to their work being used in training data\n",
    "\n",
    "üìå Concern: Ethical use of creative labor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730acc2",
   "metadata": {},
   "source": [
    "Question = 6 >>> Use the following small text dataset to train a simple Variational\n",
    "Autoencoder (VAE) for text reconstruction:\n",
    "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
    "\"The night is dark\", \"The stars are shining\"]\n",
    "1. Preprocess the data (tokenize and pad the sequences).\n",
    "2. Build a basic VAE model for text reconstruction.\n",
    "3. Train the model and show how it reconstructs or generates similar sentences.\n",
    "Include your code, explanation, and sample outputs.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa644ff",
   "metadata": {},
   "source": [
    "Ans = Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94aff22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright\",\n",
    "    \"The grass is green\",\n",
    "    \"The night is dark\",\n",
    "    \"The stars are shining\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb16768",
   "metadata": {},
   "source": [
    "1. Data Preprocessing (Tokenization & Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf90e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14\n",
      "Padded sequences:\n",
      " [[ 1  3  2  4]\n",
      " [ 1  5  2  6]\n",
      " [ 1  7  2  8]\n",
      " [ 1  9  2 10]\n",
      " [ 1 11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Dataset\n",
    "sentences = [\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright\",\n",
    "    \"The grass is green\",\n",
    "    \"The night is dark\",\n",
    "    \"The stars are shining\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Padding\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Padded sequences:\\n\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57162d",
   "metadata": {},
   "source": [
    "2. Build a Simple Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51190cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "embedding_dim = 8\n",
    "latent_dim = 4\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(max_len,))\n",
    "x = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
    "x = LSTM(16)(x)\n",
    "\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Dense(16, activation='relu')(z)\n",
    "decoder_output = Dense(max_len * vocab_size, activation='softmax')(decoder_input)\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "decoder_output = Reshape((max_len, vocab_size))(decoder_output)\n",
    "\n",
    "\n",
    "# VAE model\n",
    "vae = Model(inputs, decoder_output)\n",
    "\n",
    "# Loss\n",
    "reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    inputs, decoder_output\n",
    ")\n",
    "# Reconstruction loss\n",
    "reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    inputs, decoder_output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff3f8f",
   "metadata": {},
   "source": [
    "3. Train the Model\n",
    "4. Text Reconstruction & Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65cbac",
   "metadata": {},
   "source": [
    "Question = 7 >>> Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short\n",
    "English paragraph into French and German. Provide the original and translated text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1750ad",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "A pre-trained GPT model (such as GPT-2 or GPT-3) is a generative language model trained on large amounts of multilingual text. Although GPT models are primarily designed for text generation, they can also perform machine translation through prompt-based learning.\n",
    "\n",
    "2. How GPT Performs Translation\n",
    "\n",
    "GPT models use a prompt that clearly instructs the model to translate text from one language to another.\n",
    "\n",
    "The English paragraph is provided as input.\n",
    "\n",
    "The prompt specifies the target language (French or German).\n",
    "\n",
    "The model predicts the translated text based on learned language patterns.\n",
    "\n",
    "This method does not require additional training.\n",
    "\n",
    "3. Example\n",
    "Original English Text\n",
    "\n",
    "Artificial intelligence is transforming the world by improving healthcare, education, and communication.\n",
    "\n",
    "French Translation\n",
    "\n",
    "L‚Äôintelligence artificielle transforme le monde en am√©liorant les soins de sant√©, l‚Äô√©ducation et la communication.\n",
    "\n",
    "German Translation\n",
    "\n",
    "K√ºnstliche Intelligenz ver√§ndert die Welt, indem sie das Gesundheitswesen, die Bildung und die Kommunikation verbessert.\n",
    "\n",
    "4. Why Pre-trained GPT Works Well for Translation\n",
    "\n",
    "Learns grammar and vocabulary from large multilingual datasets\n",
    "\n",
    "Understands context and sentence structure\n",
    "\n",
    "Produces fluent and natural translations\n",
    "\n",
    "Works without task-specific fine-tuning\n",
    "\n",
    "5. Limitations\n",
    "\n",
    "GPT is not specialized only for translation\n",
    "\n",
    "May produce less accurate results for technical or rare language pairs\n",
    "\n",
    "Translation quality depends on prompt clarity\n",
    "\n",
    "6. Conclusion\n",
    "\n",
    "Pre-trained GPT models can effectively translate English text into French and German using prompt-based instructions, making them a flexible and powerful tool for multilingual text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005713b",
   "metadata": {},
   "source": [
    "Question = 8 >>>  Implement a simple attention-based encoder-decoder model for\n",
    "English-to-Spanish translation using Tensorflow or PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531608b2",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "An attention-based encoder‚Äìdecoder model is a neural network architecture used in Neural Machine Translation (NMT). It translates a sentence from a source language (English) to a target language (Spanish) by learning how words in the source sentence align with words in the target sentence.\n",
    "\n",
    "2. Encoder\n",
    "\n",
    "The encoder reads the English input sentence word by word and converts it into a sequence of hidden states using models like RNN, LSTM, or GRU.\n",
    "\n",
    "Each word is first converted into a vector using an embedding layer.\n",
    "\n",
    "The encoder produces a hidden representation for every input word.\n",
    "\n",
    "These hidden states store semantic and contextual information about the sentence.\n",
    "\n",
    "3. Attention Mechanism\n",
    "\n",
    "The attention mechanism allows the model to focus on relevant words in the English sentence while generating each Spanish word.\n",
    "\n",
    "Instead of using a single fixed context vector, attention computes weights for all encoder hidden states.\n",
    "\n",
    "Higher weights are given to more relevant words.\n",
    "\n",
    "A context vector is created as a weighted sum of encoder hidden states.\n",
    "\n",
    "This helps the model handle long sentences, word order differences, and better alignment between English and Spanish words.\n",
    "\n",
    "4. Decoder\n",
    "\n",
    "The decoder generates the Spanish translation one word at a time.\n",
    "\n",
    "It uses the context vector from attention and its own previous hidden state.\n",
    "\n",
    "At each time step, it predicts the next Spanish word using a softmax layer.\n",
    "\n",
    "The process continues until an end-of-sentence token is generated.\n",
    "\n",
    "5. Training Process\n",
    "\n",
    "The model is trained using parallel English‚ÄìSpanish sentence pairs.\n",
    "\n",
    "Teacher forcing is commonly used, where the correct previous word is given to the decoder during training.\n",
    "\n",
    "The loss function (usually categorical cross-entropy) measures how close the predicted words are to the correct translation.\n",
    "\n",
    "6. Advantages of Attention-Based Models\n",
    "\n",
    "Improves translation accuracy\n",
    "\n",
    "Handles long and complex sentences\n",
    "\n",
    "Provides better word alignment\n",
    "\n",
    "Forms the basis of Transformer models\n",
    "\n",
    "7. Conclusion\n",
    "\n",
    "An attention-based encoder‚Äìdecoder model enhances English-to-Spanish translation by dynamically focusing on the most relevant parts of the input sentence, leading to more accurate and fluent translations compared to traditional encoder‚Äìdecoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66928c9e",
   "metadata": {},
   "source": [
    "Question = 9 >>> Use the following short poetry dataset to simulate poem generation with a\n",
    "pre-trained GPT model:\n",
    "[\"Roses are red, violets are blue,\",\n",
    "\"Sugar is sweet, and so are you.\",\n",
    "\"The moon glows bright in silent skies,\",\n",
    "\"A bird sings where the soft wind sighs.\"]\n",
    "Using this dataset as a reference for poetic structure and language, generate a new 2-4\n",
    "line poem using a pre-trained GPT model (such as GPT-2). You may simulate\n",
    "fine-tuning by prompting the model with similar poetic patterns.\n",
    "Include your code, the prompt used, and the generated poem in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1eafe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_dataset = [\n",
    "    \"Roses are red, violets are blue,\",\n",
    "    \"Sugar is sweet, and so are you.\",\n",
    "    \"The moon glows bright in silent skies,\",\n",
    "    \"A bird sings where the soft wind sighs.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5cf41",
   "metadata": {},
   "source": [
    "Ans = 1. Introduction\n",
    "\n",
    "A pre-trained GPT model (like GPT-2 or GPT-3) is a generative language model trained on large text corpora. It can generate creative text such as poetry by predicting the next word in a sequence based on learned patterns.\n",
    "\n",
    "2. Using GPT for Poem Generation\n",
    "\n",
    "Step 1: Reference Dataset\n",
    "Provide a small set of poems as examples to guide the model‚Äôs style and structure.\n",
    "\n",
    "Step 2: Prompting\n",
    "Construct a prompt that instructs GPT to generate new poetic lines similar to the reference dataset.\n",
    "Example prompt:\n",
    "\n",
    "Write a 2-4 line poem similar to:\n",
    "Roses are red, violets are blue,\n",
    "Sugar is sweet, and so are you.\n",
    "\n",
    "\n",
    "Step 3: Generation\n",
    "The model uses its pre-trained knowledge of grammar, syntax, and word patterns to produce new lines that resemble the style and rhythm of the input examples.\n",
    "\n",
    "Step 4: Sampling Parameters\n",
    "\n",
    "Temperature controls creativity (higher ‚Üí more diverse output).\n",
    "\n",
    "Max length limits the number of generated tokens.\n",
    "\n",
    "Top-k / top-p sampling can reduce repetition.\n",
    "\n",
    "3. Advantages of Using GPT\n",
    "\n",
    "Can produce original poems without explicit fine-tuning.\n",
    "\n",
    "Maintains syntactic and semantic coherence.\n",
    "\n",
    "Flexible for different poetic styles by changing prompts.\n",
    "\n",
    "4. Example Output\n",
    "\n",
    "Given the reference lines:\n",
    "\n",
    "Roses are red, violets are blue,\n",
    "Sugar is sweet, and so are you.\n",
    "\n",
    "\n",
    "GPT might generate:\n",
    "\n",
    "Stars twinkle softly above the trees,\n",
    "A gentle breeze whispers through the leaves.\n",
    "Night falls quietly, the world at rest,\n",
    "Dreams awaken in every heart's chest.\n",
    "\n",
    "5. Limitations\n",
    "\n",
    "GPT may generate nonsensical or repetitive lines if the prompt is vague.\n",
    "\n",
    "Quality depends on prompt clarity and sampling settings.\n",
    "\n",
    "Not guaranteed to strictly follow meter or rhyme schemes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ca9bb",
   "metadata": {},
   "source": [
    "Question = 10 >>>  Imagine you are building a creative writing assistant for a publishing\n",
    "company. The assistant should generate story plots and character descriptions using\n",
    "Generative AI. Describe how you would design the system, including model selection,\n",
    "training data, bias mitigation, and evaluation methods. Explain the real-world challenges\n",
    "you might face.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c2649",
   "metadata": {},
   "source": [
    "Ans = 1. System Overview\n",
    "\n",
    "A creative writing assistant helps authors by generating:\n",
    "\n",
    "Story plots\n",
    "\n",
    "Character descriptions\n",
    "\n",
    "Dialogue suggestions\n",
    "\n",
    "It leverages Generative AI models, particularly large language models (LLMs), to produce coherent and imaginative content.\n",
    "\n",
    "2. Model Selection\n",
    "\n",
    "Pre-trained LLMs such as GPT-3, GPT-4, or open-source models like GPT-J / LLaMA are ideal because:\n",
    "\n",
    "They understand grammar, context, and storytelling patterns.\n",
    "\n",
    "They can generate diverse and creative outputs without extensive task-specific training.\n",
    "\n",
    "For more controlled outputs, consider fine-tuning a pre-trained model on domain-specific creative writing datasets.\n",
    "\n",
    "3. Training Data\n",
    "\n",
    "Use high-quality storytelling datasets, including:\n",
    "\n",
    "Published novels, short stories, and scripts (licensed or public domain)\n",
    "\n",
    "Character and plot descriptions\n",
    "\n",
    "Dialogue datasets\n",
    "\n",
    "Preprocess the data:\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Text cleaning\n",
    "\n",
    "Maintaining style and formatting of stories\n",
    "\n",
    "Optionally, create prompt‚Äìcompletion pairs for supervised fine-tuning.\n",
    "\n",
    "4. Bias Mitigation\n",
    "\n",
    "Generative models may replicate biases from training data. Mitigation strategies include:\n",
    "\n",
    "Data curation: Remove or balance biased content (gender, ethnicity, religion, etc.)\n",
    "\n",
    "Prompt engineering: Guide the model to produce inclusive and neutral content\n",
    "\n",
    "Post-processing filters: Detect and remove offensive or inappropriate outputs\n",
    "\n",
    "Regular audits: Check model outputs for bias or stereotypes\n",
    "\n",
    "5. Evaluation Methods\n",
    "\n",
    "Evaluate the system on multiple dimensions:\n",
    "\n",
    "a. Creativity and Coherence\n",
    "\n",
    "Human evaluators rate plots and character descriptions for originality, engagement, and logical consistency.\n",
    "\n",
    "b. Fluency and Grammar\n",
    "\n",
    "Use metrics like BLEU, ROUGE, or perplexity to measure language quality.\n",
    "\n",
    "c. Diversity\n",
    "\n",
    "Assess whether the model generates varied storylines and characters instead of repeating templates.\n",
    "\n",
    "d. Ethical and Bias Evaluation\n",
    "\n",
    "Ensure outputs are inclusive, culturally sensitive, and non-offensive.\n",
    "\n",
    "6. Real-World Challenges\n",
    "\n",
    "Bias and Offensive Content\n",
    "\n",
    "AI can produce stereotypical or inappropriate characters if not monitored.\n",
    "\n",
    "Intellectual Property Issues\n",
    "\n",
    "Generated plots may inadvertently mimic existing copyrighted works.\n",
    "\n",
    "Maintaining Coherence\n",
    "\n",
    "Long story generation may lose plot consistency without advanced techniques like memory mechanisms or hierarchical generation.\n",
    "\n",
    "Balancing Creativity vs. Control\n",
    "\n",
    "Authors may need specific plot structures or themes, requiring controlled generation methods.\n",
    "\n",
    "Evaluation Complexity\n",
    "\n",
    "Creativity and storytelling quality are subjective, making automatic evaluation difficult.\n",
    "\n",
    "7. System Architecture (High-Level)\n",
    "\n",
    "Input Module\n",
    "\n",
    "Accepts author prompts (e.g., ‚ÄúGenerate a mystery story plot with a detective‚Äù).\n",
    "\n",
    "Generative AI Module\n",
    "\n",
    "Pre-trained GPT-like model, optionally fine-tuned\n",
    "\n",
    "Generates plots, character sketches, and dialogues\n",
    "\n",
    "Bias and Safety Filter\n",
    "\n",
    "Removes offensive content, ensures inclusivity\n",
    "\n",
    "Output Module\n",
    "\n",
    "Presents multiple options\n",
    "\n",
    "Allows author feedback to refine results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
